import argparse
import os

from pyspark import SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.functions import col, collect_set, collect_list, create_map, explode, first, first_value, lower, regexp_extract, udf, when, struct
from pyspark.sql.types import StringType

# More complex URL clustering/prefix matching can be done to scope L7 authz rules
# more tightly. An example of how to extend this would be to use your services' OpenAPI
# (Swagger) specs to extract route information from the URL and use that as the prefix.
from pxspark.udfs import extract_url_prefix
from pxspark.templating import convert_row_to_dict, render_import_template, render_main_template

IGNORE_REQ_SUBSTRS = [
    "metrics", # Ignore /metrics requests
    "health",  # Ignore /health, /healthz requests
    "read",    # Ignore /ready, /readiness requests
]

def compute_authz_service_mapping(df):
    """
    compute_authz_service_mapping expects a Dataframe created from OpenTelemetry's
    opentelemetry.proto.trace.v1.ResourceSpan protobuf message type. The OTel collector
    exports this type for trace data when using a variety of its collectors. The
    file and awss3 exporters are two examples of this.
    """
    resources_df = df.select(explode(col("resourceSpans")))
    resources_df = (resources_df
                        .withColumn("resource_attrs", explode(col("col.resource.attributes")))
                        .filter(col("resource_attrs.key") == "http.xfcc_by")
                        .withColumn("xfcc_by", col("resource_attrs.value.stringValue"))
                        .withColumn("service_name", regexp_extract('xfcc_by', r'\/([\w-]+)$', 1)))
    resources_df.drop("resource_attrs")
    resources_df.drop("xfcc_by")

    df_exploded_scopespans = (resources_df
        .withColumn("scopespans_exploded", explode("col.scopeSpans")))

    df_exploded_spans = (df_exploded_scopespans
        .withColumn("spans_exploded", explode("scopespans_exploded.spans"))
        .drop("scopespans_exploded"))

    df_exploded_attributes = (df_exploded_spans
        .withColumn("span_attributes", explode("spans_exploded.attributes"))
        .withColumn("spanId", col("spans_exploded.spanId"))
        .drop("spans_exploded"))

    # Group by a unique column (spanId). This is auto generated by pixie if the span didn't contain
    # a X-B3-SpanId header.
    result = (df_exploded_attributes
        .groupBy("spanId")
        .agg(
            first(when(col("span_attributes.key") == "http.xfcc_uri", col("span_attributes.value.stringValue")), ignorenulls=True).alias("xfcc_uri"),
            first(when(col("span_attributes.key") == "http.target", extract_url_prefix(col("span_attributes.value.stringValue"))), ignorenulls=True).alias("http_target"),
            first(when(col("span_attributes.key") == "http.method", col("span_attributes.value.stringValue")), ignorenulls=True).alias("http_method"),
            first(col("service_name")).alias("service_name"))
        .filter(~ lower(col("http_target")).rlike("|".join(IGNORE_REQ_SUBSTRS)))
        .select(
            col("service_name"),
            regexp_extract('xfcc_uri', r'\/([\w-]+)$', 1).alias("client_name"),
            col("http_target"),
            col("http_method")))

    return (result.groupBy("service_name", "client_name")
            .agg(collect_set(struct("http_target", "http_method")).alias("http_target_method_map"))
            .groupBy("service_name")
            .agg(collect_list(create_map(col("client_name"), col("http_target_method_map"))).alias("client_http_target_method_map")))

def parse_args():
    parser = argparse.ArgumentParser(description="Accepts an S3 bucket name and path prefix.")

    parser.add_argument('--bucket', type=str, required=True,
                        help='The name of the S3 bucket.')
    parser.add_argument('--prefix', type=str, required=True,
                        help='The path prefix inside the S3 bucket.')

    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    conf = SparkConf()
    conf.setAll([
        ("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.2.0"),
        ("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.profile.ProfileCredentialsProvider"),
    ])

    spark = (SparkSession.builder
        .master("local[*]")
        .config(conf=conf)
        .appName("otel-export-authz-service-mapping")
        .getOrCreate())

    # The path follows the following format:
    # s3a://<bucket-name>/<prefix>/year=YYYY/month=MM/day=DD/hour=hh/min=mm/*.json
    bucket = args.bucket
    prefix = args.prefix
    df = (spark.read
              .option("header", "true")
              .json(f"s3a://{bucket}/{prefix}/*/*/*/*/*/*.json"))

    df = compute_authz_service_mapping(df)

    rows = df.collect()

    output_dir = "/home/ddelnano/code/pixie-demos/automatic-authz-envoy-opa/opa"
    # Create the directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(os.path.join(output_dir, "imports"), exist_ok=True)

    services = []
    for row in rows:
        print(row)
        service, sources = convert_row_to_dict(row)
        services.append(service)

        import_file = os.path.join(output_dir, "imports", f"{service}.rego")
        with open(import_file, "w") as f:
            import_output_backend = render_import_template(service, sources)
            f.write(import_output_backend)
    print(f"Service names: {services}")

    output_file = os.path.join(output_dir, "opa-policy.rego")

    # Write the rendered output to the file
    with open(output_file, "w") as f:
        main_output = render_main_template(services)
        f.write(main_output)
